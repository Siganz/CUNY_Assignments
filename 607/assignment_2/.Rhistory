glimpse(df_clean)
null_error_rate <- min(df$n) / sum(df$n)
sprintf("The null error rate is %.02f",(null_error_rate))
ggplot(df, aes(x = sex, y = n, fill = sex)) +
geom_col() +
labs(
title = "Distribution of Sex",
subtitle = "The count of each sex for the study.",
x = "Sex",
y = "Count"
)
df_clean <- df_raw |>
rename(
pred_female = .pred_female,
pred_class = .pred_class,
truth = sex
) |>
mutate(truth_bool = truth == "male") |>
mutate(null_error_rate = truth == "male")
glimpse(df_clean)
df_clean <- df_raw |>
rename(
pred_female = .pred_female,
pred_class = .pred_class,
truth = sex
) |>
mutate(truth_bool = truth == "male") |>
glimpse(df_clean)
df_clean <- df_raw |>
rename(
pred_female = .pred_female,
pred_class = .pred_class,
truth = sex
) |>
mutate(truth_bool = truth == "male")
glimpse(df_clean)
null_error_rate
accuracy <- 1- null_error_rate
null_error_rate
accuracy <- 1- null_error_rate
accuracy
temp <- df |>
count(sex) |>
mutate(percent = round(((n / sum(n))*100), 0))
temp
library(tidyverse)
df_raw <- read.csv(
'https://raw.githubusercontent.com/acatlin/data/refs/heads/master/penguin_predictions.csv'
)
glimpse(df_raw)
temp <- df |>
count(sex) |>
mutate(percent = round(((n / sum(n))*100), 0))
temp
glimpse(df_raw)
df <- df_raw
df <- df |>
count(sex) |>
mutate(percent = round(((n / sum(n))*100), 0))
df
null_error_rate <- min(df$n) / sum(df$n)
sprintf("The null error rate is %.02f",(null_error_rate))
ggplot(df, aes(x = sex, y = n, fill = sex)) +
geom_col() +
labs(
title = "Distribution of Sex",
subtitle = "The count of each sex for the study.",
x = "Sex",
y = "Count"
)
df_clean <- df_raw |>
rename(
pred_female = .pred_female,
pred_class = .pred_class,
truth = sex
) |>
mutate(truth_bool = truth == "male")
glimpse(df_clean)
t <- 1
threshold_prediction <- function(df_pred, t) {
pred  <- df_pred$pred_female >= t
reality <- df_pred$truth_bool
tp <-   sum(pred & reality)
fp <-   sum(pred & !reality)
tn <-   sum(!pred & !reality)
fn <-   sum(!pred & reality)
tibble(
threshold = t,
true_positive = tp,
false_positive = fp,
true_negative = tn,
false_negative = fn
)
}
accuracy <- 1- null_error_rate
confusion_plot <- function(res) {
cm <- tibble(
Actual = c("Positive","Positive","Negative","Negative"),
Pred   = c("Positive","Negative","Positive","Negative"),
n = c(
res$true_positive,
res$false_negative,
res$false_positive,
res$true_negative
)
)
ggplot(cm, aes(x = Pred, y = Actual, fill = n)) +
geom_tile() +
geom_text(aes(label = n), color = "white", size = 6) +
scale_fill_gradient(low = "grey70", high = "steelblue") +
labs(
title = "Confusion Matrix",
subtitle = sprintf("Threshold %.1f", t),
x = "Predicted",
y = "Actual"
) +
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.5),
plot.subtitle = element_text(hjust = 0.5)
)
}
t <- 0.2
c_matrix <- threshold_prediction(df_clean, t)
c_matrix
confusion_plot(c_matrix)
t <- 0.5
c_matrix <- threshold_prediction(df_clean, t)
c_matrix
confusion_plot(c_matrix)
t <- 0.8
c_matrix <- threshold_prediction(df_clean, t)
c_matrix
confusion_plot(c_matrix)
library(tidyverse)
df_raw <- read.csv(
'https://raw.githubusercontent.com/acatlin/data/refs/heads/master/penguin_predictions.csv'
)
glimpse(df_raw)
temp <- df_raw |>
count(sex) |>
mutate(percent = round(((n / sum(n))*100), 0))
temp
glimpse(df_raw)
df <- df_raw
df <- df |>
count(sex) |>
mutate(percent = round(((n / sum(n))*100), 0))
df
null_error_rate <- min(df$n) / sum(df$n)
sprintf("The null error rate is %.02f",(null_error_rate))
ggplot(df, aes(x = sex, y = n, fill = sex)) +
geom_col() +
labs(
title = "Distribution of Sex",
subtitle = "The count of each sex for the study.",
x = "Sex",
y = "Count"
)
df_clean <- df_raw |>
rename(
pred_female = .pred_female,
pred_class = .pred_class,
truth = sex
) |>
mutate(truth_bool = truth == "male")
glimpse(df_clean)
t <- 1
threshold_prediction <- function(df_pred, t) {
pred  <- df_pred$pred_female >= t
reality <- df_pred$truth_bool
tp <-   sum(pred & reality)
fp <-   sum(pred & !reality)
tn <-   sum(!pred & !reality)
fn <-   sum(!pred & reality)
tibble(
threshold = t,
true_positive = tp,
false_positive = fp,
true_negative = tn,
false_negative = fn
)
}
accuracy <- 1- null_error_rate
confusion_plot <- function(res) {
cm <- tibble(
Actual = c("Positive","Positive","Negative","Negative"),
Pred   = c("Positive","Negative","Positive","Negative"),
n = c(
res$true_positive,
res$false_negative,
res$false_positive,
res$true_negative
)
)
ggplot(cm, aes(x = Pred, y = Actual, fill = n)) +
geom_tile() +
geom_text(aes(label = n), color = "white", size = 6) +
scale_fill_gradient(low = "grey70", high = "steelblue") +
labs(
title = "Confusion Matrix",
subtitle = sprintf("Threshold %.1f", t),
x = "Predicted",
y = "Actual"
) +
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.5),
plot.subtitle = element_text(hjust = 0.5)
)
}
t <- 0.2
c_matrix <- threshold_prediction(df_clean, t)
c_matrix
confusion_plot(c_matrix)
t <- 0.5
c_matrix <- threshold_prediction(df_clean, t)
c_matrix
confusion_plot(c_matrix)
t <- 0.8
c_matrix <- threshold_prediction(df_clean, t)
c_matrix
confusion_plot(c_matrix)
accuracy <- 1- null_error_rate
accuracy <- 1- null_error_rate
accuracy
accuracy <- 1- null_error_rate
accuracy
accuracy <- function(tp, fp, tn, fn){(tp + tn) / (tp + fp + tn + fn)}
accuracy(tp, fp, tn, fn)
t <- 0.2
c_matrix <- threshold_prediction(df_clean, t)
c_matrix
confusion_plot(c_matrix)
accuracy(tp, fp, tn, fn)
accuracy <- 1- null_error_rate
accuracy <- function(tp, fp, tn, fn){(tp + tn) / (tp + fp + tn + fn)}
t <- 0.2
c_matrix <- threshold_prediction(df_clean, t)
c_matrix
confusion_plot(c_matrix)
accuracy(tp, fp, tn, fn)
t <- 0.2
c_matrix <- threshold_prediction(df_clean, t)
c_matrix
confusion_plot(c_matrix)
accuracy(cmatrix$tp, fp, tn, fn)
t <- 0.2
c_matrix <- threshold_prediction(df_clean, t)
c_matrix
confusion_plot(c_matrix)
accuracy(c_matrix$tp, fp, tn, fn)
t <- 0.2
c_matrix <- threshold_prediction(df_clean, t)
c_matrix
confusion_plot(c_matrix)
accuracy(cmatrix$true_positive, cmatrix$false_postive, cmatrix$true_negative, cmatrix$false_negative)
t <- 0.2
c_matrix <- threshold_prediction(df_clean, t)
c_matrix
confusion_plot(c_matrix)
accuracy(c_matrix$true_positive, c_matrix$false_postive, c_matrix$true_negative, c_matrix$false_negative)
t <- 0.2
c_matrix <- threshold_prediction(df_clean, t)
c_matrix
confusion_plot(c_matrix)
accuracy(c_matrix$true_positive, c_matrix$false_positive, c_matrix$true_negative, c_matrix$false_negative)
library(tidyverse)
df_raw <- read.csv(
'https://raw.githubusercontent.com/acatlin/data/refs/heads/master/penguin_predictions.csv'
)
glimpse(df_raw)
temp <- df_raw |>
count(sex) |>
mutate(percent = round(((n / sum(n))*100), 0))
temp
glimpse(df_raw)
df <- df_raw
df <- df |>
count(sex) |>
mutate(percent = round(((n / sum(n))*100), 0))
df
null_error_rate <- min(df$n) / sum(df$n)
sprintf("The null error rate is %.02f",(null_error_rate))
ggplot(df, aes(x = sex, y = n, fill = sex)) +
geom_col() +
labs(
title = "Distribution of Sex",
subtitle = "The count of each sex for the study.",
x = "Sex",
y = "Count"
)
df_clean <- df_raw |>
rename(
pred_female = .pred_female,
pred_class = .pred_class,
truth = sex
) |>
mutate(truth_bool = truth == "male")
glimpse(df_clean)
threshold_prediction <- function(df_pred, t) {
pred  <- df_pred$pred_female >= t
reality <- df_pred$truth_bool
tp <-   sum(pred & reality)
fp <-   sum(pred & !reality)
tn <-   sum(!pred & !reality)
fn <-   sum(!pred & reality)
tibble(
threshold = t,
true_positive= tp,
false_positive = fp,
true_negative = tn,
false_negative = fn
)
}
t = 1
# run threshold_prediction
p <- threshold_prediction(df, t)
t = 1
# run threshold_prediction
p <- threshold_prediction(df_clean, t)
# Setting variables
tp <- p$tp
fp <- p$fp
tn <- p$tn
fn <- p$fn
t = 1
# run threshold_prediction
p <- threshold_prediction(df, t)
library(tidyverse)
df_raw <- read.csv(
'https://raw.githubusercontent.com/acatlin/data/refs/heads/master/penguin_predictions.csv'
)
glimpse(df_raw)
temp <- df_raw |>
count(sex) |>
mutate(percent = round(((n / sum(n))*100), 0))
temp
glimpse(df_raw)
df <- df_raw
df <- df |>
count(sex) |>
mutate(percent = round(((n / sum(n))*100), 0))
df
null_error_rate <- min(df$n) / sum(df$n)
sprintf("The null error rate is %.02f",(null_error_rate))
ggplot(df, aes(x = sex, y = n, fill = sex)) +
geom_col() +
labs(
title = "Distribution of Sex",
subtitle = "The count of each sex for the study.",
x = "Sex",
y = "Count"
)
df <- df_raw |>
rename(
pred_female = .pred_female,
pred_class = .pred_class,
truth = sex
) |>
mutate(truth_bool = truth == "male")
glimpse(df)
threshold_prediction <- function(df_pred, t) {
pred  <- df_pred$pred_female >= t
reality <- df_pred$truth_bool
tp <-   sum(pred & reality)
fp <-   sum(pred & !reality)
tn <-   sum(!pred & !reality)
fn <-   sum(!pred & reality)
tibble(
threshold = t,
true_positive= tp,
false_positive = fp,
true_negative = tn,
false_negative = fn
)
}
t = 1
# run threshold_prediction
p <- threshold_prediction(df, t)
# Setting variables
tp <- p$tp
fp <- p$fp
tn <- p$tn
fn <- p$fn
accuracy <- 1- null_error_rate
accuracy <- function(tp, fp, tn, fn){(tp + tn) / (tp + fp + tn + fn)}
confusion_plot <- function(x) {
cm <- tibble(
Actual = c("Positive","Positive","Negative","Negative"),
Pred   = c("Positive","Negative","Positive","Negative"),
n = c(
x$true_positive,
x$false_negative,
x$false_positive,
x$true_negative
)
)
ggplot(cm, aes(x = Pred, y = Actual, fill = n)) +
geom_tile() +
geom_text(aes(label = n), color = "white", size = 6) +
scale_fill_gradient(low = "grey70", high = "steelblue") +
labs(
title = "Confusion Matrix",
subtitle = sprintf("Threshold %.1f", t),
x = "Predicted",
y = "Actual"
) +
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.5),
plot.subtitle = element_text(hjust = 0.5)
)
}
t <- 0.2
c_matrix <- threshold_prediction(df_clean, t)
c_matrix
confusion_plot(c_matrix)
accuracy(c_matrix$true_positive, c_matrix$false_positive, c_matrix$true_negative, c_matrix$false_negative)
t <- 0.5
c_matrix <- threshold_prediction(df_clean, t)
c_matrix
confusion_plot(c_matrix)
t <- 0.8
c_matrix <- threshold_prediction(df_clean, t)
c_matrix
confusion_plot(c_matrix)
t = 1
# run threshold_prediction
p <- threshold_prediction(df, t)
# Setting variables
p
t = 1
# run threshold_prediction
p <- threshold_prediction(df, t)
# Setting variables
tp <- p$tp
fp <- p$fp
tn <- p$tn
fn <- p$fn
t = 1
# run threshold_prediction
p <- threshold_prediction(df, t)
# Setting variables
tp
t = 1
# run threshold_prediction
p <- threshold_prediction(df, t)
# Setting variables
tp
fp
t = 1
# run threshold_prediction
p <- threshold_prediction(df, t)
# Setting variables
tp
fp
t = 1
# run threshold_prediction
p <- threshold_prediction(df, t)
# Setting variables
tp <- p$true_positive
fp <- p$false_positive
tn <- p$true_negative
fn <- p$false_negative
t = 1
# run threshold_prediction
p <- threshold_prediction(df, t)
# Setting variables
tp <- p$true_positive
fp <- p$false_positive
tn <- p$true_negative
fn <- p$false_negative
tp, fp, tn, fn
t = 1
# run threshold_prediction
p <- threshold_prediction(df, t)
# Setting variables
tp <- p$true_positive
fp <- p$false_positive
tn <- p$true_negative
fn <- p$false_negative
tp
t = 1
# run threshold_prediction
p <- threshold_prediction(df, t)
# Setting variables
tp <- p$true_positive
fp <- p$false_positive
tn <- p$true_negative
fn <- p$false_negative
tp
fp
tn
fn
accuracy <- 1- null_error_rate
accuracy <- function(tp, fp, tn, fn){(tp + tn) / (tp + fp + tn + fn)}
a <- 1- null_error_rate
a
accuracy <- function(tp, fp, tn, fn){(tp + tn) / (tp + fp + tn + fn)}
a <- 1- null_error_rate
a
accuracy <- function(tp, fp, tn, fn){(tp + tn) / (tp + fp + tn + fn)}
a
a <- 1- null_error_rate
a
accuracy <- function(tp, fp, tn, fn){(tp + tn) / (tp + fp + tn + fn)}
acuracy
a <- 1- null_error_rate
a
a <- function(tp, fp, tn, fn){(tp + tn) / (tp + fp + tn + fn)}
a
